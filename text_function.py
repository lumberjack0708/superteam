import re
import json
import time
from datetime import datetime
import time
from bs4 import BeautifulSoup
from selenium import webdriver

with open ('keyword_url.json','r',encoding='utf-8') as f:
    url_dict = f.read()
    #è½‰ç‚ºpython
    url_dict = json.loads(url_dict)

with open ('keyword.json','r',encoding='utf-8') as f:
    url_list = f.read()
    url_list = json.loads(url_list)

# é—œéµè©è¾¨èªåŠå›å¾©ç›¸é—œè³‡è¨Š
def main_text(text,predict=False):
    if predict == True:
        return f"ä½¿ç”¨è€…çš„å•é¡Œ:ç•¶ä¸‹å¤©æ°£ç‹€æ³;\né æ¸¬çµæœ:{text};\nå¹«æˆ‘ç”Ÿæˆç°¡å–®å›æ‡‰ä¸¦çµ¦äºˆå»ºè­°"
    else:
        is_get = False
        for i in range(len(url_list)):
            if re.search(url_list[i],text):
                is_get = True
                result = url_list[i]
                break
        if is_get == True:
            refresh = time_space(get_info_from_json(result))
            if refresh == True:
                res = get_specia_reply(result)
                res = cobi_test(text,result,res)
                return res,is_get
            else:
                res = get_info(result)
                res = cobi_test(text,result,res)
                return res,is_get
        else:
            print(f"return message: {text}")
            res = text
            return res,is_get

# é—œéµè©æœç´¢åŠçˆ¬èŸ²è³‡æ–™å„²å­˜   
def get_specia_reply(key):
    from URL_load import scrape_data
    url = url_dict[key][0]
    xpath = url_dict[key][1]
    re_inf = scrape_data(url,xpath)
    # print(f"å·²å–å¾—è³‡è¨Š{re_inf}")
    information_store = get_from_json()
    information_store[key]["info"] = re_inf
    information_store[key]["time"] = time.strftime("%Y-%m-%d %H:%M")
    store_to_json(information_store)
    return re_inf

def cobi_test(text,key,info):
    return f"ä½¿ç”¨è€…çš„å•é¡Œ:{text};\nä»¥ä¸‹æ˜¯ç›®å‰å·²çŸ¥çš„{key}è³‡è¨Šï¼š\n{info}ã€‚\nå¹«æˆ‘ä»¥ä¸€èˆ¬äººé¡å°è©±çš„æ–¹å¼å›æ‡‰"

def store(text):
    with open('store.txt','w',encoding='utf-8') as f:
        f.write(text)
        f.close()

# æ™‚é–“é–“éš”åˆ¤æ–·
def time_space(info:dict):
    need = False
    t1 = info["time"]
    space = info["space"]
    time.sleep(1)
    t2 = time.strftime("%Y-%m-%d %H:%M")
    t1 = datetime.strptime(t1, "%Y-%m-%d %H:%M")
    t2 = datetime.strptime(t2, "%Y-%m-%d %H:%M")
    t_diff = t2 - t1
    t_minute = int(t_diff.total_seconds()/60)
    if t_minute >= space*60:
        need = True
    return need

#  å–å¾—æ‰€æœ‰è³‡è¨Šå„²å­˜
def get_from_json():
    with open ('information_store.json','r',encoding='utf-8') as f:
        data = f.read()
        data = json.loads(data)
    return data

def get_info_from_json(title:str):
    information_store = get_from_json()
    if title in information_store:
        return information_store[title]
    else:
        return None
    
def store_to_json(data:dict):
    data = json.dumps(data,ensure_ascii=False,indent=4)
    with open ('information_store.json','w',encoding='utf-8') as f:
        f.write(data)
        f.close()

# å–å¾—å„²å­˜çš„è³‡è¨Š
def get_info(key):
    information_store = get_from_json()
    if key in information_store:
        return information_store[key]["info"]
    
def get_from_store():
    with open('store.txt','r',encoding='utf-8') as f:
        text = f.read()
        f.close()
    return text

# åœ°å€å°æ‡‰çš„ CID å­—å…¸
CID_MAPPING = {
    "åŸºéš†å¸‚": "11017",
    "å°åŒ—å¸‚": "63",
    "è‡ºåŒ—å¸‚": "63",
    "æ–°åŒ—å¸‚": "65",
    "æ¡ƒåœ’å¸‚": "68",
    "æ–°ç«¹å¸‚": "10018",
    "æ–°ç«¹ç¸£": "10004",
    "è‹—æ —ç¸£": "10005",
    "å°ä¸­å¸‚": "66",
    "å½°åŒ–ç¸£": "10007",
    "å—æŠ•ç¸£": "10008",
    "é›²æ—ç¸£": "10009",
    "å˜‰ç¾©å¸‚": "10020",
    "å˜‰ç¾©ç¸£": "10010",
    "å°å—å¸‚": "67",
    "é«˜é›„å¸‚": "64",
    "å±æ±ç¸£": "10013",
    "å®œè˜­ç¸£": "10002",
    "èŠ±è“®ç¸£": "10015",
    "æ¾æ¹–ç¸£": "10016",
    "é‡‘é–€ç¸£": "09020",
    "é€£æ±Ÿç¸£": "09007"
}

def scrape_table_selenium(cid):
    url = f"https://www.cwa.gov.tw/V8/C/W/County/County.html?CID={cid}"
    driver = webdriver.Chrome()
    driver.get(url)
    time.sleep(3)  # ç­‰å¾… JavaScript åŠ è¼‰å®Œæˆ
    html = driver.page_source
    driver.quit()
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", {"id": "PC_Week_MOD", "class": "table table-bordered"})
    if not table:
        return []

    rows = table.find_all("tr")
    table_data = []
    for row in rows:
        cells = row.find_all(["td", "th"])
        row_data = []
        for cell in cells:
            celsius_span = cell.find("span", class_="tem-C is-active")
            if celsius_span:
                row_data.append(celsius_span.get_text(strip=True))
            else:
                row_data.append(cell.get_text(strip=True))
        table_data.append(row_data)
    return table_data

def format_weather_output(table_data):
    if not table_data or len(table_data) < 2:
        return "è¡¨æ ¼è³‡æ–™ä¸è¶³ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

    location = table_data[0][0]
    dates = table_data[0][1:]
    output = f"{location} - ğŸŒ¤ï¸ æœªä¾†ä¸€å‘¨å¤©æ°£é å ±\n\n"

    for date_idx, date in enumerate(dates):
        output += f"ğŸ—“ï¸ {date}\n"
        for row in table_data[1:]:
            label = row[0]
            value = row[date_idx + 1]
            if label == "ç™½å¤©":
                emoji = "â˜€ï¸"
            elif label == "æ™šä¸Š":
                emoji = "ğŸŒ™"
            elif label == "é«”æ„Ÿæº«åº¦":
                emoji = "ğŸŒ¡ï¸"
            elif label == "ç´«å¤–ç·š":
                emoji = "ğŸŒ"
            else:
                emoji = ""
            output += f"{emoji} {label}: {value}\n"
        output += "\n"
    return output

def get_weather_forecast(location):
    cid = CID_MAPPING.get(location)
    if not cid:
        return "åœ°å€åç¨±ç„¡æ•ˆï¼Œè«‹é‡æ–°è¼¸å…¥ã€‚"

    table_data = scrape_table_selenium(cid)
    if not table_data:
        return "ç„¡æ³•å–å¾—å¤©æ°£è³‡æ–™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    return format_weather_output(table_data)